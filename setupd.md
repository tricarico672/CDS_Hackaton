# Running locally

To make sure that the model is able to run locally first start a chat with mistral by running in a separate terminal
`ollama run mistral`

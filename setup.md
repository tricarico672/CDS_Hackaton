# Running locally

To make sure that the model is able to run locally first start a chat with mistral by running in a separate terminal
`ollama run mistral`

# Running using the Mistral API

To run the model using the mistral API make sure that you configure a `.env` file setting the `API-KEY` variable to your Mistral API key.
